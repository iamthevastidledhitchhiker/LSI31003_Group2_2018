{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "havedisplay = \"DISPLAY\" in os.environ\n",
    "if havedisplay:\n",
    "    matplotlib.use('TkAgg')\n",
    "else:\n",
    "    matplotlib.use('Agg')\n",
    "    \n",
    "from training import *\n",
    "from sklearn import decomposition\n",
    "import math\n",
    "import ScatterHist as sh\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import Combat\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalle/LSI31003GrOOp2/src/training.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer = l2(l2_penalty)\n",
      "/home/kalle/LSI31003GrOOp2/src/training.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"linear\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer = l2(l2_penalty)\n",
      "/home/kalle/LSI31003GrOOp2/src/training.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  autoencoder = Model(input = input_cell, output = decoded)\n"
     ]
    }
   ],
   "source": [
    "day1, day2 = load_data(\"../Data/Person2Day1_baseline.csv\", \"../Data/Person2Day2_baseline.csv\", use_denoise = True, denoise_params = { \"verbose\": True })\n",
    "#apparently there are very big differences between the datasets...\n",
    "#this was clustering quite well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting scales using KNN\n",
      "[0.6840974829921764, 1.3681949659843529, 2.7363899319687057]\n",
      "setting all scale weights to 1\n",
      "Train on 3877 samples, validate on 431 samples\n",
      "Epoch 1/10\n",
      "3877/3877 [==============================] - 13s 3ms/step - loss: 0.5215 - val_loss: 0.4019\n",
      "Epoch 2/10\n",
      "3877/3877 [==============================] - 12s 3ms/step - loss: 0.4002 - val_loss: 0.4148\n",
      "Epoch 3/10\n",
      "3877/3877 [==============================] - 11s 3ms/step - loss: 0.3533 - val_loss: 0.3914\n",
      "Epoch 4/10\n",
      "3877/3877 [==============================] - 12s 3ms/step - loss: 0.3234 - val_loss: 0.3880\n",
      "Epoch 5/10\n",
      "3877/3877 [==============================] - 12s 3ms/step - loss: 0.3065 - val_loss: 0.3834\n",
      "Epoch 6/10\n",
      "3877/3877 [==============================] - 13s 3ms/step - loss: 0.2960 - val_loss: 0.3887\n",
      "Epoch 7/10\n",
      "3877/3877 [==============================] - 12s 3ms/step - loss: 0.2884 - val_loss: 0.4168\n",
      "Epoch 8/10\n",
      "3877/3877 [==============================] - 12s 3ms/step - loss: 0.2791 - val_loss: 0.3825\n",
      "Epoch 9/10\n",
      "3877/3877 [==============================] - 11s 3ms/step - loss: 0.2801 - val_loss: 0.4064\n",
      "Epoch 10/10\n",
      "3877/3877 [==============================] - 11s 3ms/step - loss: 0.2680 - val_loss: 0.3974\n"
     ]
    }
   ],
   "source": [
    "(_, last_block), mmd_net = create_network(target.shape[1])\n",
    "#epochs  should be quite large?\n",
    "train_network(mmd_net, day1, day2, last_block, verbose = True, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#day1 is adjusted to resemble day2\n",
    "adj_day1 = apply_network(mmd_net, day1)\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(np.concatenate([day1, day2]))\n",
    "\n",
    "pca_day1 = pca.transform(day1)\n",
    "pca_day2 = pca.transform(day2)\n",
    "pca_adj1 = pca.transform(adj_day1)\n",
    "\n",
    "# choose PCs to plot\n",
    "pc1 = 0\n",
    "pc2 = 1\n",
    "axis1 = 'PC'+str(pc1)\n",
    "axis2 = 'PC'+str(pc2)\n",
    "sh.scatterHist(pca_day2[:,pc1], pca_day2[:,pc2], pca_day1[:,pc1], pca_day1[:,pc2], axis1, axis2, \"before calibration\")\n",
    "sh.scatterHist(pca_day2[:,pc1], pca_day2[:,pc2], pca_adj1[:,pc1], pca_adj1[:,pc2], axis1, axis2, \"after calibration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some functions .........\n",
    "\n",
    "#with resnet we adjust day1 to fit day2 data\n",
    "#if combat is used then adjusted day2 should also be used\n",
    "#day is a tuple (data, labels)\n",
    "def visualize_results(day1, day2, adj_day1, adj_day2=[], dataset_name=\"\"):\n",
    "    pca = decomposition.PCA()\n",
    "    \n",
    "    #determine the principal components using day1 and day2 data\n",
    "    #these will be used for all plots \n",
    "    pca.fit(np.concatenate([day1[0], day2[0]]))\n",
    "    pca_day1 = pca.transform(day1[0])\n",
    "    pca_day2 = pca.transform(day2[0])\n",
    "    pca_adj1 = pca.transform(adj_day1[0])\n",
    "    \n",
    "    #make the labels a bit more informative\n",
    "    labels_1 = append_strings(day1[1], \"day1, \")\n",
    "    labels_2 = append_strings(day2[1], \"day2, \")\n",
    "    adj_labels_1 = append_strings(day1[1], \"adjusted day1, \")\n",
    "    \n",
    "    q = np.concatenate([pca_day1, pca_day2])\n",
    "    plot_clusters(q, np.concatenate([labels_1, labels_2]), title=dataset_name+\" before calibration\")\n",
    "    \n",
    "    if(len(adj_day2) == 0):\n",
    "        q = np.concatenate([pca_adj1, pca_day2])\n",
    "        plot_clusters(q, np.concatenate([adj_labels_1, labels_2]), title=dataset_name+\" after calibration\")\n",
    "        visualize_transition(pca_day1, pca_day2, pca_adj1, title=dataset_name)\n",
    "    else:\n",
    "        pca_adj2 = pca.transform(adj_day2[0])\n",
    "        adj_labels_2 = append_strings(day2[1], \"adjusted_day2, \")\n",
    "        q = np.concatenate([pca_adj1, pca_adj2])\n",
    "        plot_clusters(q, np.concatenate([adj_labels_1, adj_labels_2]), title=dataset_name+\" after calibration\")\n",
    "        visualize_transition(pca_day1, pca_day2, pca_adj1, pca_adj2, visualize_n=75, title=dataset_name)\n",
    "\n",
    "#cluster_labels[i] is the label of the cluster containing data[i]\n",
    "def plot_clusters(data, cluster_labels, title=\"\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    \n",
    "    colormap = plt.get_cmap('tab20')\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    color = 0\n",
    "    for x in unique_labels:\n",
    "        label_indexes = (np.array(cluster_labels) == x)\n",
    "        n = sum(label_indexes)\n",
    "        ax.scatter(data[label_indexes,0], data[label_indexes, 1], color=colormap(n*[color]), label=x, marker='.')\n",
    "        color += 1\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#visualizes the differences between original and adjusted days\n",
    "#draws lines (or arrows) between visualize_n random data points\n",
    "#parameters are pca so that the visualization can be compared with the other plots\n",
    "def visualize_transition(pca_day1, pca_day2, pca_adj1, pca_adj2=[], visualize_n=150, title=\"\", arrows=True):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(pca_day1[:,0], pca_day1[:,1], label='day1', color='r', marker='o', alpha=0.6)\n",
    "    ax.scatter(pca_day2[:,0], pca_day2[:,1], label='day2', color='b', marker='o', alpha=0.6)\n",
    "    ax.scatter(pca_adj1[:,0], pca_adj1[:,1], label='day1 adjusted', color='g', marker='o', alpha=0.6)\n",
    "    \n",
    "\n",
    "    #select visualize_n random points and draw the lines\n",
    "    n = min(len(pca_day1), visualize_n)\n",
    "    selected = np.random.choice(range(len(pca_day1)), n, replace=0)\n",
    "    for p1, p2 in zip(pca_day1[selected], pca_adj1[selected]):\n",
    "        x = p1[0]\n",
    "        y = p1[1]\n",
    "        dx = p2[0]-p1[0]\n",
    "        dy = p2[1]-p1[1]\n",
    "        if arrows == True:\n",
    "            ax.arrow(x, y, dx, dy, head_width=0.1, alpha=1, fc='m', linestyle='dotted', length_includes_head=True)\n",
    "        else:\n",
    "            ax.plot([p1[0], p2[0]], [p1[1], p2[1]], 'k--')\n",
    "    \n",
    "    if len(pca_adj2) > 0: \n",
    "        ax.scatter(pca_adj2[:,0], pca_adj2[:,1], label='day2 adjusted', color='m', marker='o', alpha=0.6)\n",
    "\n",
    "        #select visualize_n random points and draw the lines\n",
    "        n = min(len(pca_day1), visualize_n)\n",
    "        selected = np.random.choice(range(len(pca_day2)), n, replace=0)\n",
    "        if arrows == True:\n",
    "            for p1, p2 in zip(pca_day2[selected], pca_adj2[selected]):\n",
    "                x = p1[0]\n",
    "                y = p1[1]\n",
    "                dx = p2[0]-p1[0]\n",
    "                dy = p2[1]-p1[1]\n",
    "                if arrows == True:\n",
    "                    ax.arrow(x, y, dx, dy, head_width=0.1,alpha=1, fc='b',linestyle='solid', length_includes_head=True)\n",
    "                else:\n",
    "                    ax.plot([p1[0], p2[0]], [p1[1], p2[1]], 'k--')\n",
    "        \n",
    "    ax.legend(loc='upper right')\n",
    "    plt.title(title + \", transition\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "    \n",
    "def append_strings(label, x):\n",
    "    return np.array(list(map(lambda y: str(x) + str(y), label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also TSNE, maybe it is not so important...\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=1000, learning_rate=500)\n",
    "\n",
    "#uncomment if necessary\n",
    "#tsne_transition = tsne.fit_transform(np.concatenate([day2, day1, adj_day1]))\n",
    "\n",
    "visualize_transition(tsne_transition[len(day2):-len(adj_day1)], tsne_transition[:len(day2)], tsne_transition[-len(adj_day1):], title=\"T-SNE source and calibrated source\", arrows=False)\n",
    "#quite messy, doesnt have clear clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plot three first 3 principal components\n",
    "def plot_clusters_3d(pca_data, cluster_labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    colormap = plt.get_cmap('tab20')\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    color = 0\n",
    "    for x in unique_labels:\n",
    "        label_indexes = (np.array(cluster_labels) == x)\n",
    "        n = sum(label_indexes)\n",
    "        ax.scatter(pca_data[label_indexes,0], pca_data[label_indexes, 1], pca_data[label_indexes, 2], color=colormap(n*[color]), label=x, marker='.')\n",
    "        color += 1\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "#sort and divide data according to the clusters\n",
    "def sort_clusters(data, cluster_labels):\n",
    "    clusters = []\n",
    "    n = len(np.unique(cluster_labels))\n",
    "    for i in range(n):\n",
    "        clusters.append(data[cluster_labels == i])\n",
    "    return clusters\n",
    "\n",
    "\n",
    "#rename clusters so that the cluster sizes are in increasing order\n",
    "#just to make it a bit easier to select somewhat similar clusters\n",
    "#between different kmeans runs\n",
    "def fix_cluster_ids(cluster_id):\n",
    "    cluster_cnt = Counter()\n",
    "    for i in range(len(cluster_id)):\n",
    "        cluster_cnt[cluster_id[i]] += 1\n",
    "    cluster_order = list(sorted(cluster_cnt, key=cluster_cnt.get))\n",
    "    for i in range(len(cluster_id)):\n",
    "        cluster_id[i] = cluster_order.index(cluster_id[i])\n",
    "    return cluster_id\n",
    "\n",
    "def k_means(data, n_clusters):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    #cluster_id[i] is the cluster of data[i]\n",
    "    cluster_id = model.fit_predict(data)\n",
    "    #kmeans is run several times to improve the result\n",
    "    for i in range(10):\n",
    "        model2 = KMeans(n_clusters=n_clusters)\n",
    "        cluster_id_2 = model2.fit_predict(data)\n",
    "        if model2.inertia_ < model.inertia_:\n",
    "            model = model2\n",
    "            cluster_id = cluster_id_2\n",
    "\n",
    "    #the labels are sort so that the clusters' sizes form an increasing sequence\n",
    "    return fix_cluster_ids(cluster_id)\n",
    "  \n",
    "\n",
    "###############\n",
    "#Do we actually know that the algorithm is working correctly?\n",
    "#We know that the distribution of adj_day1 maps to day2 pretty well but is that enough?\n",
    "#Does each cluster map to \"correct\" cluster?\n",
    "#How can we know that the clusters are not mixing? For example one cluster from day1 shouldn't map to \n",
    "#two clusters from day2\n",
    "\n",
    "#Here the batch corrected data (adjusted day1 and day2) is clustered. \n",
    "#Two clusters are selected. Based on these clusters, data from day1 and day2\n",
    "#are selected. \n",
    "\n",
    "#How is this helpful? Now we have only two clusters that we are trying to map to two clusters.\n",
    "#We can perhaps see with PCA if the clusters are mapping to a correct cluster. \n",
    "#At least we can see if the ResNet mixes the clusters.\n",
    "\n",
    "#With these two clusters we can also see how the ResNet performs when the corresponding clusters have\n",
    "#different sizes on different days i.e. the batches are unbalanced.\n",
    "\n",
    "#Of course, here we assume that the ResNet works at least to some extent\n",
    "#(we select the clusters based on ResNet adjusted data...)\n",
    "\n",
    "\n",
    "#Additionally it would be nice to something like bootstrapping with ResNet. \n",
    "#If it is working correctly, it should map each individual point to (almost) same place\n",
    "#every time.\n",
    "#################\n",
    "\n",
    "\n",
    "combined_data = np.concatenate([day2, adj_day1])\n",
    "\n",
    "#8 seemed to be ok...\n",
    "#it's not so important here because we just want to have some clusters\n",
    "kmeans_clusters = k_means(combined_data, 8)\n",
    "\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=9)\n",
    "agg_clusters = agg.fit_predict(combined_data)\n",
    "agg_clusters = fix_cluster_ids(agg_clusters)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=1000, learning_rate=500)\n",
    "\n",
    "#uncomment if necessary\n",
    "#combined_tsne = tsne.fit_transform(combined_data)\n",
    "\n",
    "combined_data_pca = pca.fit_transform(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "#combined data, tsne\n",
    "plot_clusters(combined_tsne, kmeans_clusters, title=\"kmeans clustering, batch effect corrected\")\n",
    "plot_clusters(combined_tsne, agg_clusters, title=\"agglomerative clustering, batch effect corrected\")\n",
    "\n",
    "#choose agglomerative clustering\n",
    "combined_data_clusters = agg_clusters\n",
    "\n",
    "plot_clusters_3d(combined_data_pca, combined_data_clusters)\n",
    "#ok, so the clusters are not very clear..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the data in clusters\n",
    "#cell_populations_target[i] is a list of cells belonging to cluster i and to the \"target\" sample\n",
    "cell_populations_day2 = sort_clusters(day2, combined_data_clusters[:len(target)])\n",
    "cell_populations_day1 = sort_clusters(day1, combined_data_clusters[len(target):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#are the clusters separated in a similar way in day1 and day2?\n",
    "#here it would be good to have a better clustering method\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=1000, learning_rate=100)\n",
    "#uncomment if necessary\n",
    "#day1_tsne = tsne.fit_transform(source)\n",
    "#day2_tsne = tsne.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(day1_tsne, combined_data_clusters[len(target):], title=\"clusters on unprocessed day1 data\")\n",
    "plot_clusters(day2_tsne, combined_data_clusters[:len(target)], title=\"clusters on unprocessed day2 data\")\n",
    "#seems ok... or... i dont know...\n",
    "#clusers from ResNet corrected data seem to form clusters also on unprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we make a new day1 and day2 samples\n",
    "#we select only two cell populations and make the batches somewhat unbalanced\n",
    "#we compare these results to a mapping where the batches are balanced\n",
    "\n",
    "#take distribution[i] datapoints from the cluster number i\n",
    "def prepare_dataset(clusters, distribution):\n",
    "    dataset = []\n",
    "    cluster_labels = []\n",
    "    while len(distribution) < len(clusters):\n",
    "        distribution.append(0)\n",
    "        \n",
    "    for i in range(len(clusters)):        \n",
    "        indexes = np.random.choice(range(len(clusters[i])), distribution[i])\n",
    "        for j in range(distribution[i]):\n",
    "            dataset.append(clusters[i][indexes[j]])\n",
    "            cluster_labels.append(i)\n",
    "            \n",
    "    return (np.asarray(dataset), cluster_labels)\n",
    "\n",
    "\n",
    "#here I select clusters number 7 and 8\n",
    "#note that cell population sizes are different so maybe the \"distribution\" values should be\n",
    "#adjusted a bit...\n",
    "#balanced\n",
    "distribution = [0]*len(cell_populations_day1)\n",
    "distribution[4] = 200\n",
    "distribution[6] = 200\n",
    "balanced_day1 = prepare_dataset(cell_populations_day1, distribution)\n",
    "balanced_day2 = prepare_dataset(cell_populations_day2, distribution)\n",
    "\n",
    "\n",
    "#\"unbalanced\" dataset\n",
    "distribution[4] = 150\n",
    "distribution[6] = 250\n",
    "unbalanced_day1 = prepare_dataset(cell_populations_day1, distribution)\n",
    "distribution[4] = 250\n",
    "distribution[6] = 150\n",
    "unbalanced_day2 = prepare_dataset(cell_populations_day2, distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting scales using KNN\n",
      "[0.6041567735855703, 1.2083135471711406, 2.416627094342281]\n",
      "setting all scale weights to 1\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "def calibrate_data(day1, day2, n):\n",
    "    (_, last_block), mmd_net = create_network(day2[0].shape[1])\n",
    "    train_network(mmd_net, day1[0], day2[0], last_block)\n",
    "    return (apply_network(mmd_net, day1[0]), day1[1],n)\n",
    "\n",
    "(_, last_block2), mmd_net2 = create_network(balanced_day1[0].shape[1])\n",
    "\n",
    "#uncomment....\n",
    "train_network(mmd_net2, balanced_day1[0], balanced_day2[0], last_block2, epochs=120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(data, cluster_labels)\n",
    "balanced_day1_adj = (apply_network(mmd_net2, balanced_day1[0]), balanced_day1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting scales using KNN\n",
      "[0.5812092028455123, 1.1624184056910245, 2.324836811382049]\n",
      "setting all scale weights to 1\n"
     ]
    }
   ],
   "source": [
    "(_, last_block3), mmd_net3 = create_network(unbalanced_day1[0].shape[1])\n",
    "\n",
    "#uncomment\n",
    "train_network(mmd_net3, unbalanced_day1[0], unbalanced_day2[0], last_block3, epochs=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(data, cluster_labels)\n",
    "unbalanced_day1_adj = (apply_network(mmd_net3, unbalanced_day1[0]), unbalanced_day1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Comparing with Combat\n",
    "def combat_adjust(day1, day2):\n",
    "    pheno = ['day1']*len(day1) + ['day2']*len(day2)\n",
    "    data = pd.DataFrame(np.concatenate([day1, day2]).T)\n",
    "    ebat = Combat.combat(data, pheno, None)\n",
    "    return ebat\n",
    "\n",
    "def visualize_combat(day1, day2, dataset_name=''):\n",
    "    combat2 = combat_adjust(day1[0], day2[0]).as_matrix().T\n",
    "    combat_day1 = combat2[:len(day1[0])]\n",
    "    combat_day2 = combat2[len(day1[0]):]\n",
    "    l1 = [0]*len(day1[0])\n",
    "    l2 = [0]*len(day2[0])\n",
    "    visualize_results(day1, day2, (combat_day1, l1), (combat_day2, l2), dataset_name=dataset_name+\",combat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "found 2 batches\n",
      "found 0 numerical covariates...\n",
      "found 0 categorical variables:\t\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors\n",
      "Finding parametric adjustments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "visualize_results(balanced_day1, balanced_day2, balanced_day1_adj, dataset_name=\"'balanced'\")\n",
    "visualize_combat(balanced_day1, balanced_day2, 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "found 2 batches\n",
      "found 0 numerical covariates...\n",
      "found 0 categorical variables:\t\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors\n",
      "Finding parametric adjustments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting data\n"
     ]
    }
   ],
   "source": [
    "visualize_results(unbalanced_day1, unbalanced_day2, unbalanced_day1_adj, dataset_name=\"'unbalanced'\")\n",
    "visualize_combat(unbalanced_day1, unbalanced_day2, 'unbalanced')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
